{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2f83179-3173-4136-933b-25757ec5e874",
   "metadata": {},
   "source": [
    "# Main Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "84f7538b-08ca-40dd-bb3f-cdac7df5f7aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading column 1408\n",
      "Reading column 1442\n",
      "GPU decompression\n",
      "Output buffer size 62544\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Libraries\n",
    "from kvikio.nvcomp_codec import NvCompBatchCodec\n",
    "from kvikio import defaults, CuFile\n",
    "import uproot\n",
    "import numpy as np\n",
    "import cupy as cp\n",
    "import awkward as ak\n",
    "from uproot.models.RNTuple import _recursive_find\n",
    "\n",
    "def GPU_read_col_col_cluster_pages(in_ntuple, ncol, cluster_i, filehandle):\n",
    "    linklist = in_ntuple.page_list_envelopes.pagelinklist[cluster_i]\n",
    "    pagelist = linklist[ncol].pages if ncol < len(linklist) else []\n",
    "    dtype_byte = in_ntuple.column_records[ncol].type\n",
    "    dtype_str = uproot.const.rntuple_col_num_to_dtype_dict[dtype_byte]\n",
    "    total_len = np.sum([desc.num_elements for desc in pagelist], dtype=int)\n",
    "    if dtype_str == \"switch\":\n",
    "        dtype = np.dtype([(\"index\", \"int64\"), (\"tag\", \"int32\")])\n",
    "    elif dtype_str == \"bit\":\n",
    "        dtype = np.dtype(\"bool\")\n",
    "    else:\n",
    "        dtype = np.dtype(dtype_str)\n",
    "    full_output_buffer = cp.empty(total_len, dtype = dtype)    \n",
    "    split = dtype_byte in uproot.const.rntuple_split_types\n",
    "    \n",
    "\n",
    "    n_pages = len(pagelist)\n",
    "    output_buffers = []\n",
    "    compressed_buffers = []\n",
    "    futures = []\n",
    "\n",
    "    tracker = 0\n",
    "    nelements_tracker = 0\n",
    "    for page_desc in pagelist:\n",
    "        n_elements = page_desc.num_elements\n",
    "        \n",
    "        \n",
    "        loc = page_desc.locator\n",
    "        n_bytes = loc.num_bytes\n",
    "        isbit = dtype_str == \"bit\"\n",
    "        len_divider = 8 if isbit else 1\n",
    "        num_elements = n_elements\n",
    "        if isbit:\n",
    "            num_elements_toread = int(numpy.ceil(num_elements / 8))\n",
    "        elif dtype_str in (\"real32trunc\", \"real32quant\"):\n",
    "            num_elements_toread = int(numpy.ceil((num_elements * 4 * nbits) / 32))\n",
    "            dtype = numpy.dtype(\"uint8\")\n",
    "        else:\n",
    "            num_elements_toread = num_elements\n",
    "        nelements_tracker += num_elements_toread\n",
    "        tracker_end = tracker + n_elements\n",
    "        uncomp_size = num_elements_toread * dtype.itemsize\n",
    "        out_buff = full_output_buffer[tracker:num_elements_toread]\n",
    "\n",
    "        # Use locator to read page\n",
    "        comp_buff = cp.empty(n_bytes - 9, dtype = \"b\")\n",
    "        fut = filehandle.pread(comp_buff,\n",
    "                              size = int(n_bytes - 9),\n",
    "                              file_offset = int(loc.offset + 9))\n",
    "\n",
    "\n",
    "        output_buffers.append(out_buff)\n",
    "        compressed_buffers.append(comp_buff)\n",
    "        futures.append(fut)\n",
    "    # Because some columns contain extra bits in the compressed buffer that \n",
    "    # get 'chopped' off, need to check that total_len == sum(num_elements_to_read)\n",
    "    # for prototype for now. Only relevant for cols with (isBit = True)\n",
    "    assert(total_len == nelements_tracker)\n",
    "    \n",
    "    for future in futures:\n",
    "        future.get()\n",
    "            \n",
    "    return (compressed_buffers, output_buffers, futures)\n",
    "    \n",
    "        \n",
    "    \n",
    "def GPU_read_col_clusters(in_ntuple, ncol, cluster_range, dtype_byte):\n",
    "    filepath = in_ntuple.file.source.file_path\n",
    "    compressed_buffers = []\n",
    "    output_buffers = []\n",
    "    futures = []\n",
    "    print(\"Reading column {}\".format(ncol))\n",
    "    with CuFile(filepath, \"rb\") as filehandle:\n",
    "        for i, cluster_i in enumerate(cluster_range):\n",
    "            (cluster_compressed_buffers,\n",
    "             cluster_output_buffers,\n",
    "             cluster_futures) = GPU_read_col_col_cluster_pages(in_ntuple,\n",
    "                                                                ncol,\n",
    "                                                                cluster_i,\n",
    "                                                                filehandle)\n",
    "            # Aggregate results\n",
    "            compressed_buffers.extend(cluster_compressed_buffers)\n",
    "            output_buffers.extend(cluster_output_buffers)\n",
    "            futures.extend(cluster_futures)\n",
    "\n",
    "    # Does not work.... futures maybe cannot be passed arbitrarily deep\n",
    "    # Wait for all reads to complete\n",
    "        # for fut in futures:\n",
    "        #     if not fut.done():\n",
    "        #         print(f\"Future not completed: {fut}\")\n",
    "        #         fut.get()  \n",
    "\n",
    "    return compressed_buffers, output_buffers\n",
    "    \n",
    "\n",
    "def GPU_read_cols(in_ntuple, columns, start_cluster_idx, stop_cluster_idx):\n",
    "    compressed_buffers = []\n",
    "    output_buffers = []\n",
    "    for key in columns:\n",
    "        if \"column\" in key and \"union\" not in key:\n",
    "            key_nr = int(key.split(\"-\")[1])\n",
    "            dtype_byte = in_ntuple.ntuple.column_records[key_nr].type\n",
    "            cluster_range = range(start_cluster_idx, stop_cluster_idx)\n",
    "            \n",
    "            compressed_buffers_col, output_buffers_col = GPU_read_col_clusters(in_ntuple,\n",
    "                                                                       key_nr,\n",
    "                                                                       cluster_range,\n",
    "                                                                       dtype_byte)\n",
    "            compressed_buffers.extend(compressed_buffers_col)\n",
    "            output_buffers.extend(output_buffers_col)\n",
    "            \n",
    "    #print(len(compressed_buffers), len(output_buffers))\n",
    "    #print(compressed_buffers, output_buffers)\n",
    "    return(compressed_buffers, output_buffers)\n",
    "            \n",
    "\n",
    "def Process_decompressed_content(in_ntuple, columns,\n",
    "                                 start_cluster_idx, stop_cluster_idx,\n",
    "                                 all_decompressed_content):\n",
    "    for key in columns:\n",
    "        n_pages = 0\n",
    "\n",
    "        if \"column\" in key and \"union\" not in key:\n",
    "            key_nr = int(key.split(\"-\")[1])\n",
    "            dtype_byte = in_ntuple.ntuple.column_records[key_nr].type\n",
    "            cluster_range = range(start_cluster_idx, stop_cluster_idx)\n",
    "            page_i = -1\n",
    "            \n",
    "            for i in cluster_range:\n",
    "                arrays = []\n",
    "                linklist = self.page_list_envelopes.pagelinklist[cluster_i]\n",
    "                pagelist = linklist[ncol].pages if ncol < len(linklist) else []\n",
    "                n_pages += len(pagelist)\n",
    "                dtype_byte = self.column_records[ncol].type\n",
    "                dtype_str = uproot.const.rntuple_col_num_to_dtype_dict[dtype_byte]\n",
    "                total_len = np.sum([desc.num_elements for desc in pagelist], dtype=int)\n",
    "                if dtype_str == \"switch\":\n",
    "                    dtype = numpy.dtype([(\"index\", \"int64\"), (\"tag\", \"int32\")])\n",
    "                elif dtype_str == \"bit\":\n",
    "                    dtype = numpy.dtype(\"bool\")\n",
    "                else:\n",
    "                    dtype = numpy.dtype(dtype_str)\n",
    "                res = cp.empty(total_len, dtype)\n",
    "                split = dtype_byte in uproot.const.rntuple_split_types\n",
    "                zigzag = dtype_byte in uproot.const.rntuple_zigzag_types\n",
    "                delta = dtype_byte in uproot.const.rntuple_delta_types\n",
    "                index = dtype_byte in uproot.const.rntuple_index_types\n",
    "                nbits = uproot.const.rntuple_col_num_to_size_dict[dtype_byte]\n",
    "                tracker = 0\n",
    "                cumsum = 0\n",
    "                for page_desc in pagelist:\n",
    "                    page_i += 1\n",
    "                    n_elements = page_desc.num_elements\n",
    "                    tracker_end = tracker + n_elements\n",
    "                    content = all_decompressed_content[i]\n",
    "\n",
    "                    loc = page_desc.locator\n",
    "                    context = {}\n",
    "                    # bool in RNTuple is always stored as bits\n",
    "                    isbit = dtype_str == \"bit\"\n",
    "                    len_divider = 8 if isbit else 1\n",
    "\n",
    "                    if split:\n",
    "                        content = content.view(cp.uint8)\n",
    "            \n",
    "                        if nbits == 16:\n",
    "                            # AAAAABBBBB needs to become\n",
    "                            # ABABABABAB\n",
    "                            res = cp.empty(len(content), cp.uint8)\n",
    "                            res[0::2] = content[len(res) * 0 // 2 : len(res) * 1 // 2]\n",
    "                            res[1::2] = content[len(res) * 1 // 2 : len(res) * 2 // 2]\n",
    "            \n",
    "                        elif nbits == 32:\n",
    "                            # AAAAABBBBBCCCCCDDDDD needs to become\n",
    "                            # ABCDABCDABCDABCDABCD\n",
    "                            res = cp.empty(len(content), cp.uint8)\n",
    "                            res[0::4] = content[len(res) * 0 // 4 : len(res) * 1 // 4]\n",
    "                            res[1::4] = content[len(res) * 1 // 4 : len(res) * 2 // 4]\n",
    "                            res[2::4] = content[len(res) * 2 // 4 : len(res) * 3 // 4]\n",
    "                            res[3::4] = content[len(res) * 3 // 4 : len(res) * 4 // 4]\n",
    "            \n",
    "                        elif nbits == 64:\n",
    "                            # AAAAABBBBBCCCCCDDDDDEEEEEFFFFFGGGGGHHHHH needs to become\n",
    "                            # ABCDEFGHABCDEFGHABCDEFGHABCDEFGHABCDEFGH\n",
    "                            res = cp.empty(len(content), cp.uint8)\n",
    "                            res[0::8] = content[len(res) * 0 // 8 : len(res) * 1 // 8]\n",
    "                            res[1::8] = content[len(res) * 1 // 8 : len(res) * 2 // 8]\n",
    "                            res[2::8] = content[len(res) * 2 // 8 : len(res) * 3 // 8]\n",
    "                            res[3::8] = content[len(res) * 3 // 8 : len(res) * 4 // 8]\n",
    "                            res[4::8] = content[len(res) * 4 // 8 : len(res) * 5 // 8]\n",
    "                            res[5::8] = content[len(res) * 5 // 8 : len(res) * 6 // 8]\n",
    "                            res[6::8] = content[len(res) * 6 // 8 : len(res) * 7 // 8]\n",
    "                            res[7::8] = content[len(res) * 7 // 8 : len(res) * 8 // 8]\n",
    "            \n",
    "                        content[:] = res.view(dtype)\n",
    "            \n",
    "                    if isbit:\n",
    "                        content = (\n",
    "                            cp.unpackbits(content.view(dtype=numpy.uint8))\n",
    "                            .reshape(-1, 8)[:, ::-1]\n",
    "                            .reshape(-1)\n",
    "                        )\n",
    "                    elif dtype_str in (\"real32trunc\", \"real32quant\"):\n",
    "                        if nbits == 32:\n",
    "                            content = content.view(numpy.uint32)\n",
    "                        elif nbits % 8 == 0:\n",
    "                            new_content = numpy.zeros((num_elements, 4), numpy.uint8)\n",
    "                            nbytes = nbits // 8\n",
    "                            new_content[:, :nbytes] = content.reshape(-1, nbytes)\n",
    "                            content = new_content.view(numpy.uint32).reshape(-1)\n",
    "                        else:\n",
    "                            ak = uproot.extras.awkward()\n",
    "                            vm = ak.forth.ForthMachine32(\n",
    "                                f\"\"\"input x output y uint32 {num_elements} x #{nbits}bit-> y\"\"\"\n",
    "                            )\n",
    "                            vm.run({\"x\": content})\n",
    "                            content = vm[\"y\"]\n",
    "                        if dtype_str == \"real32trunc\":\n",
    "                            content <<= 32 - nbits\n",
    "            \n",
    "                    # needed to chop off extra bits incase we used `unpackbits`\n",
    "                    content = content[:num_elements]\n",
    "\n",
    "                    if delta:\n",
    "                        content[0] -= cumsum\n",
    "                        cumsum += cp.sum(content[:])\n",
    "                    tracker = tracker_end\n",
    "\n",
    "                if index:\n",
    "                    res = cp.insert(res, 0, 0)  # for offsets\n",
    "                if zigzag:\n",
    "                    res = _from_zigzag(res)\n",
    "                elif delta:\n",
    "                    res = cp.cumsum(res)\n",
    "\n",
    "                arrays.append(res)\n",
    "        \n",
    "        # Check if column stores offset values for jagged arrays (splitindex64) (applies to cardinality cols too):\n",
    "        if dtype_byte in uproot.const.rntuple_delta_types:\n",
    "            # Extract the last offset values:\n",
    "            last_elements = [\n",
    "                arr[-1] for arr in arrays[:-1]\n",
    "            ]  # First value always zero, therefore skip first arr.\n",
    "            # Compute cumulative sum using itertools.accumulate:\n",
    "            last_offsets = list(accumulate(last_elements))\n",
    "            # Add the offsets to each array\n",
    "            for i in range(1, len(arrays)):\n",
    "                arrays[i] += last_offsets[i - 1]\n",
    "            # Remove the first element from every sub-array except for the first one:\n",
    "            arrays = [arrays[0]] + [arr[1:] for arr in arrays[1:]]\n",
    "\n",
    "        res = cp.concatenate(arrays, axis=0)\n",
    "\n",
    "        if pad_missing_element:\n",
    "            first_element_index = self.column_records[ncol].first_element_index\n",
    "            res = numpy.pad(res, (first_element_index, 0))\n",
    "                    \n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "def kvikuproot_open_RNTuple(in_ntuple_path, columns, classname, entry_start = 0, entry_stop = None):\n",
    "    in_ntuple = uproot.open(in_ntuple_path)[classname]\n",
    "    entry_stop = entry_stop or in_ntuple.ntuple.num_entries\n",
    "    \n",
    "    # Find clusters to read that contain data from entry_start to entry_stop\n",
    "    clusters = in_ntuple.ntuple.cluster_summaries\n",
    "    cluster_starts = np.array([c.num_first_entry for c in clusters])\n",
    "\n",
    "    start_cluster_idx = (\n",
    "        np.searchsorted(cluster_starts, entry_start, side=\"right\") - 1\n",
    "    )\n",
    "    stop_cluster_idx = np.searchsorted(cluster_starts, entry_stop, side=\"right\")\n",
    "    cluster_num_entries = np.sum(\n",
    "        [c.num_entries for c in clusters[start_cluster_idx:stop_cluster_idx]]\n",
    "    )\n",
    "\n",
    "    # Get form for requested columns\n",
    "    form = in_ntuple.to_akform().select_columns(\n",
    "        columns, prune_unions_and_records=False\n",
    "    )\n",
    "\n",
    "    # Only read columns mentioned in the awkward form\n",
    "    target_cols = []\n",
    "    container_dict = {}\n",
    "    _recursive_find(form, target_cols)\n",
    "\n",
    "    # Read all columns 'compressed' data\n",
    "    all_compressed_buffers, all_output_buffers = GPU_read_cols(in_ntuple,\n",
    "                                                             target_cols,\n",
    "                                                             start_cluster_idx,\n",
    "                                                             stop_cluster_idx)\n",
    "    # Decompression GPU\n",
    "    print(\"GPU decompression\")\n",
    "    codec = NvCompBatchCodec(\"zstd\")\n",
    "    print(\"Output buffer size\", all_output_buffers[0].nbytes)\n",
    "    all_decompressed_content = codec.decode_batch(all_compressed_buffers,\n",
    "                                                  all_output_buffers)\n",
    "\n",
    "    # Process decompressed data\n",
    "    # arrays = Process_decompressed_content(in_ntuple,\n",
    "                                          # target_cols,\n",
    "                                          # start_cluster_idx,\n",
    "                                          # stop_cluster_idx,\n",
    "                                          # all_decompressed_content)\n",
    "    \n",
    "\n",
    "    # Process decompressed data\n",
    "    # for key in target_cols:\n",
    "    #     if \"column\" in key and \"union\" not in key:\n",
    "    #         key_nr = int(key.split(\"-\")[1])\n",
    "    #         dtype_byte = in_ntuple.ntuple.column_records[key_nr].type\n",
    "    #         cluster_range = range(start_cluster_idx, stop_cluster_idx)\n",
    "    #         arrays = []\n",
    "    #         for i, cluster_i in enumerate(cluster_range):\n",
    "    #                 linklist = self.page_list_envelopes.pagelinklist[cluster_i]\n",
    "    #                 pagelist = linklist[ncol].pages if ncol < len(linklist) else []\n",
    "    #                 dtype_byte = self.column_records[ncol].type\n",
    "    #                 dtype_str = uproot.const.rntuple_col_num_to_dtype_dict[dtype_byte]\n",
    "    #                 total_len = numpy.sum([desc.num_elements for desc in pagelist], dtype=int)\n",
    "    #                 if dtype_str == \"switch\":\n",
    "    #                     dtype = numpy.dtype([(\"index\", \"int64\"), (\"tag\", \"int32\")])\n",
    "    #                 elif dtype_str == \"bit\":\n",
    "    #                     dtype = numpy.dtype(\"bool\")\n",
    "    #                 else:\n",
    "    #                     dtype = numpy.dtype(dtype_str)\n",
    "    #                 split = dtype_byte in uproot.const.rntuple_split_types\n",
    "    #                 zigzag = dtype_byte in uproot.const.rntuple_zigzag_types\n",
    "    #                 delta = dtype_byte in uproot.const.rntuple_delta_types\n",
    "    #                 index = dtype_byte in uproot.const.rntuple_index_types\n",
    "    #                 nbits = uproot.const.rntuple_col_num_to_size_dict[dtype_byte]\n",
    "    #                 tracker = 0\n",
    "    #                 cumsum = 0\n",
    "    #                 for page_desc in pagelist:\n",
    "    #                     n_elements = page_desc.num_elements\n",
    "    #                     tracker_end = tracker + n_elements\n",
    "    #                     self.read_pagedesc(\n",
    "    #                         res[tracker:tracker_end], page_desc, dtype_str, dtype, nbits, split\n",
    "    #                     )\n",
    "    #                     if delta:\n",
    "    #                         res[tracker] -= cumsum\n",
    "    #                         cumsum += cp.sum(res[tracker:tracker_end])\n",
    "    #                     tracker = tracker_end\n",
    "            \n",
    "    #                 if index:\n",
    "    #                     res = cp.insert(res, 0, 0)  # for offsets\n",
    "    #                 if zigzag:\n",
    "    #                     res = _from_zigzag(res)\n",
    "    #                 elif delta:\n",
    "    #                     res = cp.cumsum(res)\n",
    "\n",
    "                    \n",
    "                \n",
    "        \n",
    "        \n",
    "        \n",
    "    return 0\n",
    "\n",
    "path_rntuple = \"/home/fstrug/uscmshome/nobackup/GPU/kvikio_playground/TTToSemiLeptonic_UL18JMERNTuple-zstd.root\"\n",
    "cols = [\"Jet_pt\"]\n",
    "classname = \"Events\"\n",
    "kvikuproot_open_RNTuple(path_rntuple, cols, classname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f55a53-7ee3-464b-a35b-20945d03bde2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a33e86d0-ad8b-4ad2-9545-29f7bafacdd4",
   "metadata": {},
   "source": [
    "# Fluff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c64de4a-09e0-481e-92e9-bd274e5c8c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "array = cp.array([1], dtype = cp.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c96d71-fa6d-48ca-bf5d-f4cf623012e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee88281c-a624-4c3f-b536-18875c7ce84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "byte_val = b'P\\xf4\\x00'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d71105d7-fdb4-4582-a152-1cbf6a6e860c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62544\n"
     ]
    }
   ],
   "source": [
    "int_val = int.from_bytes(byte_val, byteorder = 'little')\n",
    "print(int_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "77e98409-cdd1-4288-8485-52f33092769d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 3 4]\n"
     ]
    }
   ],
   "source": [
    "array = cp.array([1,2,3,4])\n",
    "array_ptr1 = array[0:2]\n",
    "array_ptr2 = array[2:4]\n",
    "array_ptr1[0] = 2\n",
    "print(array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3f0ea73c-0259-43c4-9d3f-957b5f967aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "list = [array_ptr1, array_ptr2]\n",
    "list[0][0] = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fede5988-bedc-4fe2-9cf0-061af07e88d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 2, 3, 4])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb104a78-89e7-4268-aca9-1e7e024bbd34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-kvikio-cuda12.5-py3.11]",
   "language": "python",
   "name": "conda-env-.conda-kvikio-cuda12.5-py3.11-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
