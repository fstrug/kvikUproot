{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2f83179-3173-4136-933b-25757ec5e874",
   "metadata": {},
   "source": [
    "# Main Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84f7538b-08ca-40dd-bb3f-cdac7df5f7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "from kvikio.nvcomp_codec import NvCompBatchCodec\n",
    "from kvikio import defaults, CuFile\n",
    "import uproot\n",
    "import numpy as np\n",
    "import numpy\n",
    "import cupy as cp\n",
    "import awkward as ak\n",
    "\n",
    "def _recursive_find(form, res):\n",
    "    ak = uproot.extras.awkward()\n",
    "\n",
    "    if hasattr(form, \"form_key\"):\n",
    "        if form.form_key not in res:\n",
    "            res.append(form.form_key)\n",
    "    if hasattr(form, \"contents\"):\n",
    "        for c in form.contents:\n",
    "            _recursive_find(c, res)\n",
    "    if hasattr(form, \"content\") and issubclass(type(form.content), ak.forms.Form):\n",
    "        _recursive_find(form.content, res)\n",
    "\n",
    "def GPU_read_col_cluster_pages(in_ntuple, ncol, cluster_i, filehandle):\n",
    "    linklist = in_ntuple.page_list_envelopes.pagelinklist[cluster_i]\n",
    "    pagelist = linklist[ncol].pages if ncol < len(linklist) else []\n",
    "    dtype_byte = in_ntuple.column_records[ncol].type\n",
    "    dtype_str = uproot.const.rntuple_col_num_to_dtype_dict[dtype_byte]\n",
    "    total_len = np.sum([desc.num_elements for desc in pagelist], dtype=int)\n",
    "    if dtype_str == \"switch\":\n",
    "        dtype = np.dtype([(\"index\", \"int64\"), (\"tag\", \"int32\")])\n",
    "    elif dtype_str == \"bit\":\n",
    "        dtype = np.dtype(\"bool\")\n",
    "    else:\n",
    "        dtype = np.dtype(dtype_str)\n",
    "    full_output_buffer = cp.empty(total_len, dtype = dtype)    \n",
    "    split = dtype_byte in uproot.const.rntuple_split_types\n",
    "    \n",
    "\n",
    "    n_pages = len(pagelist)\n",
    "    output_buffers = []\n",
    "    compressed_buffers = []\n",
    "    futures = []\n",
    "\n",
    "    tracker = 0\n",
    "    nelements_tracker = 0\n",
    "    for page_desc in pagelist:\n",
    "        n_elements = page_desc.num_elements\n",
    "        loc = page_desc.locator\n",
    "        n_bytes = loc.num_bytes\n",
    "        isbit = dtype_str == \"bit\"\n",
    "        len_divider = 8 if isbit else 1\n",
    "        num_elements = n_elements\n",
    "        if isbit:\n",
    "            num_elements_toread = int(numpy.ceil(num_elements / 8))\n",
    "        elif dtype_str in (\"real32trunc\", \"real32quant\"):\n",
    "            num_elements_toread = int(numpy.ceil((num_elements * 4 * nbits) / 32))\n",
    "            dtype = numpy.dtype(\"uint8\")\n",
    "        else:\n",
    "            num_elements_toread = num_elements\n",
    "        nelements_tracker += num_elements_toread\n",
    "        uncomp_size = num_elements_toread * dtype.itemsize\n",
    "        tracker_end = tracker + num_elements_toread\n",
    "        out_buff = full_output_buffer[tracker:tracker_end]\n",
    "        # print(\"Page {} base buffer ID: \".format(i) ,id(out_buff.base))\n",
    "        # Use locator to read page\n",
    "        comp_buff = cp.empty(n_bytes - 9, dtype = \"b\")\n",
    "        fut = filehandle.pread(comp_buff,\n",
    "                              size = int(n_bytes - 9),\n",
    "                              file_offset = int(loc.offset + 9))\n",
    "\n",
    "\n",
    "        output_buffers.append(out_buff)\n",
    "        compressed_buffers.append(comp_buff)\n",
    "        futures.append(fut)\n",
    "        tracker = tracker_end\n",
    "    # Because some columns contain extra bits in the compressed buffer that \n",
    "    # get 'chopped' off, need to check that total_len == sum(num_elements_to_read)\n",
    "    # for prototype for now. Only relevant for cols with (isBit = True)\n",
    "    # assert(total_len == nelements_tracker)\n",
    "    \n",
    "    #for future in futures:\n",
    "    #    future.get()\n",
    "            \n",
    "    return (compressed_buffers, output_buffers, futures, full_output_buffer)\n",
    "    \n",
    "        \n",
    "    \n",
    "def GPU_read_col_clusters(in_ntuple, ncol, cluster_range, dtype_byte):\n",
    "    filepath = in_ntuple.file.source.file_path\n",
    "    compressed_buffers = []\n",
    "    output_buffers = []\n",
    "    futures = []\n",
    "    cluster_output_buffers = []\n",
    "    #print(\"Reading column {}\".format(ncol))\n",
    "    with CuFile(filepath, \"rb\") as filehandle:\n",
    "        for i, cluster_i in enumerate(cluster_range):\n",
    "            (cluster_compressed_buffers,\n",
    "             page_output_buffers,\n",
    "             cluster_futures,\n",
    "             cluster_full_output_buffer) = GPU_read_col_cluster_pages(in_ntuple,\n",
    "                                                                ncol,\n",
    "                                                                cluster_i,\n",
    "                                                                filehandle)\n",
    "            # print(\"Cluster {} output buffer ID after return\".format(cluster_i), id(cluster_full_output_buffer))\n",
    "            # print(\"Page base output buffer ID after return\", id(page_output_buffers[0].base))\n",
    "            # Aggregate results\n",
    "            compressed_buffers.extend(cluster_compressed_buffers)\n",
    "            output_buffers.extend(page_output_buffers)\n",
    "            futures.extend(cluster_futures)\n",
    "            cluster_output_buffers.append(cluster_full_output_buffer)\n",
    "            # print(\"Cluster {} output buffer ID after append into aggregate list\".format(cluster_i), id(cluster_output_buffers[0]))\n",
    "            # print(\"Page base output buffer ID after extend into aggregate list\", id(output_buffers[0].base))\n",
    "        for future in futures:\n",
    "            future.get()\n",
    "    return (compressed_buffers, output_buffers, cluster_output_buffers)\n",
    "    \n",
    "\n",
    "def GPU_read_cols(in_ntuple, columns, start_cluster_idx, stop_cluster_idx):\n",
    "    compressed_buffers = []\n",
    "    all_page_output_buffers = []\n",
    "    all_cluster_output_buffers = []\n",
    "    for key in columns:\n",
    "        if \"column\" in key and \"union\" not in key:\n",
    "            key_nr = int(key.split(\"-\")[1])\n",
    "            dtype_byte = in_ntuple.ntuple.column_records[key_nr].type\n",
    "            cluster_range = range(start_cluster_idx, stop_cluster_idx)\n",
    "            \n",
    "            (compressed_buffers_col,\n",
    "             page_output_buffers_col,\n",
    "             cluster_output_buffers)= GPU_read_col_clusters(in_ntuple,\n",
    "                                                                       key_nr,\n",
    "                                                                       cluster_range,\n",
    "                                                                       dtype_byte)\n",
    "            compressed_buffers.extend(compressed_buffers_col)\n",
    "            all_page_output_buffers.extend(page_output_buffers_col)\n",
    "            all_cluster_output_buffers.extend(cluster_output_buffers)\n",
    "            \n",
    "    #print(len(compressed_buffers), len(output_buffers))\n",
    "    #print(compressed_buffers, output_buffers)\n",
    "    return(compressed_buffers, all_page_output_buffers, all_cluster_output_buffers)\n",
    "\n",
    "\n",
    "def process_page_decompressed_buffer(destination, desc, dtype_str, dtype, nbits, split):\n",
    "    context = {}\n",
    "    # bool in RNTuple is always stored as bits\n",
    "    isbit = dtype_str == \"bit\"\n",
    "    num_elements = len(destination)\n",
    "    if split:\n",
    "        content = cp.copy(destination).view(cp.uint8)\n",
    "\n",
    "        if nbits == 16:\n",
    "            # AAAAABBBBB needs to become\n",
    "            # ABABABABAB\n",
    "            res = cp.empty(len(content), cp.uint8)\n",
    "            res[0::2] = content[len(res) * 0 // 2 : len(res) * 1 // 2]\n",
    "            res[1::2] = content[len(res) * 1 // 2 : len(res) * 2 // 2]\n",
    "\n",
    "        elif nbits == 32:\n",
    "            # AAAAABBBBBCCCCCDDDDD needs to become\n",
    "            # ABCDABCDABCDABCDABCD\n",
    "            res = cp.empty(len(content), cp.uint8)\n",
    "            res[0::4] = content[len(res) * 0 // 4 : len(res) * 1 // 4]\n",
    "            res[1::4] = content[len(res) * 1 // 4 : len(res) * 2 // 4]\n",
    "            res[2::4] = content[len(res) * 2 // 4 : len(res) * 3 // 4]\n",
    "            res[3::4] = content[len(res) * 3 // 4 : len(res) * 4 // 4]\n",
    "\n",
    "        elif nbits == 64:\n",
    "            # AAAAABBBBBCCCCCDDDDDEEEEEFFFFFGGGGGHHHHH needs to become\n",
    "            # ABCDEFGHABCDEFGHABCDEFGHABCDEFGHABCDEFGH\n",
    "            res = cp.empty(len(content), cp.uint8)\n",
    "            res[0::8] = content[len(res) * 0 // 8 : len(res) * 1 // 8]\n",
    "            res[1::8] = content[len(res) * 1 // 8 : len(res) * 2 // 8]\n",
    "            res[2::8] = content[len(res) * 2 // 8 : len(res) * 3 // 8]\n",
    "            res[3::8] = content[len(res) * 3 // 8 : len(res) * 4 // 8]\n",
    "            res[4::8] = content[len(res) * 4 // 8 : len(res) * 5 // 8]\n",
    "            res[5::8] = content[len(res) * 5 // 8 : len(res) * 6 // 8]\n",
    "            res[6::8] = content[len(res) * 6 // 8 : len(res) * 7 // 8]\n",
    "            res[7::8] = content[len(res) * 7 // 8 : len(res) * 8 // 8]\n",
    "\n",
    "        content = res.view(dtype)\n",
    "\n",
    "    if isbit:\n",
    "        content = (\n",
    "            cp.unpackbits(content.view(dtype=cp.uint8))\n",
    "            .reshape(-1, 8)[:, ::-1]\n",
    "            .reshape(-1)\n",
    "        )\n",
    "    elif dtype_str in (\"real32trunc\", \"real32quant\"):\n",
    "        if nbits == 32:\n",
    "            content = content.view(cp.uint32)\n",
    "        elif nbits % 8 == 0:\n",
    "            new_content = cp.zeros((num_elements, 4), cp.uint8)\n",
    "            nbytes = nbits // 8\n",
    "            new_content[:, :nbytes] = content.reshape(-1, nbytes)\n",
    "            content = new_content.view(cp.uint32).reshape(-1)\n",
    "        else:\n",
    "            ak = uproot.extras.awkward()\n",
    "            vm = ak.forth.ForthMachine32(\n",
    "                f\"\"\"input x output y uint32 {num_elements} x #{nbits}bit-> y\"\"\"\n",
    "            )\n",
    "            vm.run({\"x\": content})\n",
    "            content = vm[\"y\"]\n",
    "        if dtype_str == \"real32trunc\":\n",
    "            content <<= 32 - nbits\n",
    "\n",
    "    # needed to chop off extra bits incase we used `unpackbits`\n",
    "    destination[:] = content[:num_elements]\n",
    "\n",
    "\n",
    "\n",
    "def Process_decompressed_content(in_ntuple, columns,\n",
    "                                 start_cluster_idx, stop_cluster_idx,\n",
    "                                 all_cluster_decompressed_buffers):\n",
    "\n",
    "    cluster_range = range(start_cluster_idx, stop_cluster_idx)\n",
    "    n_clusters = stop_cluster_idx - start_cluster_idx\n",
    "    col_arrays = {} # collect content for each col\n",
    "    j = 0\n",
    "    for key in columns:\n",
    "        if \"column\" in key and \"union\" not in key:\n",
    "            # Get decompressed array for key for all clusters\n",
    "            col_decompressed_buffers = all_cluster_decompressed_buffers[j*(n_clusters):(j+1)*n_clusters]\n",
    "            j += 1\n",
    "            key_nr = int(key.split(\"-\")[1])\n",
    "            #print(\"Processing key {} buffers\".format(key_nr))\n",
    "            dtype_byte = in_ntuple.ntuple.column_records[key_nr].type\n",
    "            arrays = []\n",
    "            ncol = key_nr\n",
    "            \n",
    "            for i in cluster_range:\n",
    "                # Get decompressed buffer corresponding to cluster i\n",
    "                cluster_buffer = col_decompressed_buffers[i]\n",
    "                \n",
    "                # Get pagelist and metadatas\n",
    "                linklist = in_ntuple.page_list_envelopes.pagelinklist[i]\n",
    "                pagelist = linklist[ncol].pages if ncol < len(linklist) else []\n",
    "                dtype_byte = in_ntuple.column_records[ncol].type\n",
    "                dtype_str = uproot.const.rntuple_col_num_to_dtype_dict[dtype_byte]\n",
    "                total_len = np.sum([desc.num_elements for desc in pagelist], dtype=int)\n",
    "                if dtype_str == \"switch\":\n",
    "                    dtype = cp.dtype([(\"index\", \"int64\"), (\"tag\", \"int32\")])\n",
    "                elif dtype_str == \"bit\":\n",
    "                    dtype = cp.dtype(\"bool\")\n",
    "                else:\n",
    "                    dtype = cp.dtype(dtype_str)\n",
    "                split = dtype_byte in uproot.const.rntuple_split_types\n",
    "                zigzag = dtype_byte in uproot.const.rntuple_zigzag_types\n",
    "                delta = dtype_byte in uproot.const.rntuple_delta_types\n",
    "                index = dtype_byte in uproot.const.rntuple_index_types\n",
    "                nbits = uproot.const.rntuple_col_num_to_size_dict[dtype_byte]\n",
    "                \n",
    "                # Begin looping through pages\n",
    "                tracker = 0\n",
    "                cumsum = 0\n",
    "                for page_desc in pagelist:\n",
    "                    n_elements = page_desc.num_elements\n",
    "                    tracker_end = tracker + n_elements\n",
    "                    \n",
    "                    # Get content associated with page\n",
    "                    page_buffer = cluster_buffer[tracker:tracker_end]\n",
    "                    process_page_decompressed_buffer(page_buffer,\n",
    "                                                    page_desc,\n",
    "                                                    dtype_str,\n",
    "                                                    dtype,\n",
    "                                                    nbits,\n",
    "                                                    split)\n",
    "\n",
    "                    if delta:\n",
    "                        cluster_buffer[tracker] -= cumsum\n",
    "                        cumsum += cp.sum(cluster_buffer[tracker:tracker_end])\n",
    "                    tracker = tracker_end\n",
    "\n",
    "                if index:\n",
    "                    cluster_buffer = cupy_insert0(cluster_buffer)  # for offsets\n",
    "                if zigzag:\n",
    "                    cluster_buffer = _from_zigzag(cluster_buffer)\n",
    "                elif delta:\n",
    "                    cluster_buffer = cp.cumsum(cluster_buffer)\n",
    "                elif dtype_str == \"real32trunc\":\n",
    "                    cluster_buffer = cluster_buffer.view(cp.float32)\n",
    "                elif dtype_str == \"real32quant\" and ncol < len(self.column_records):\n",
    "                    min_value = self.column_records[ncol].min_value\n",
    "                    max_value = self.column_records[ncol].max_value\n",
    "                    cluster_content = min_value + cluster_content.astype(cp.float32) * (max_value - min_value) / (\n",
    "                        (1 << nbits) - 1\n",
    "                    )\n",
    "                    cluster_buffer = cluster_buffer.astype(cp.float32)\n",
    "                arrays.append(cluster_buffer)\n",
    "\n",
    "            if dtype_byte in uproot.const.rntuple_delta_types:\n",
    "                # Extract the last offset values:\n",
    "                last_elements = [\n",
    "                    arr[-1].get() for arr in arrays[:-1]\n",
    "                ]  # First value always zero, therefore skip first arr.\n",
    "                # Compute cumulative sum using itertools.accumulate:\n",
    "                last_offsets = np.cumsum(last_elements)\n",
    "                \n",
    "                # Add the offsets to each array\n",
    "                for i in range(1, len(arrays)):\n",
    "                    arrays[i] += last_offsets[i - 1]\n",
    "                # Remove the first element from every sub-array except for the first one:\n",
    "                arrays = [arrays[0]] + [arr[1:] for arr in arrays[1:]]\n",
    "    \n",
    "            res = cp.concatenate(arrays, axis=0)\n",
    "    \n",
    "            if True:\n",
    "                first_element_index = in_ntuple.column_records[ncol].first_element_index\n",
    "                res = cp.pad(res, (first_element_index, 0))\n",
    "            \n",
    "            col_arrays[key_nr] = res\n",
    "    \n",
    "    return col_arrays\n",
    "\n",
    "def cupy_insert0(arr):\n",
    "    #Intended for flat cupy arrays\n",
    "    array_len = arr.shape[0]\n",
    "    array_dtype = arr.dtype\n",
    "    out_arr = cp.empty(array_len + 1, dtype = array_dtype)\n",
    "    cp.copyto(out_arr[1:], arr)\n",
    "    out_arr[0] = 0\n",
    "    return(out_arr)\n",
    "                    \n",
    "\n",
    "            \n",
    "def kvikuproot_open_RNTuple(in_ntuple_path, columns, classname, prototype = True, entry_start = 0, entry_stop = None):\n",
    "    in_ntuple = uproot.open(in_ntuple_path)[classname]\n",
    "    entry_stop = entry_stop or in_ntuple.ntuple.num_entries\n",
    "    \n",
    "    # Find clusters to read that contain data from entry_start to entry_stop\n",
    "    clusters = in_ntuple.ntuple.cluster_summaries\n",
    "    cluster_starts = np.array([c.num_first_entry for c in clusters])\n",
    "\n",
    "    start_cluster_idx = (\n",
    "        np.searchsorted(cluster_starts, entry_start, side=\"right\") - 1\n",
    "    )\n",
    "    stop_cluster_idx = np.searchsorted(cluster_starts, entry_stop, side=\"right\")\n",
    "    cluster_num_entries = np.sum(\n",
    "        [c.num_entries for c in clusters[start_cluster_idx:stop_cluster_idx]]\n",
    "    )\n",
    "\n",
    "    # Get form for requested columns\n",
    "    form = in_ntuple.to_akform().select_columns(\n",
    "        columns, prune_unions_and_records=False\n",
    "    )\n",
    "\n",
    "    # Only read columns mentioned in the awkward form\n",
    "    target_cols = []\n",
    "    container_dict = {}\n",
    "    _recursive_find(form, target_cols)\n",
    "\n",
    "\n",
    "    #print(\"Begin reading\")\n",
    "    # Read all columns 'compressed' data\n",
    "    (all_compressed_buffers,\n",
    "     all_output_buffers,\n",
    "     all_cluster_output_buffers) = GPU_read_cols(in_ntuple,\n",
    "                                                 target_cols,\n",
    "                                                 start_cluster_idx,\n",
    "                                                 stop_cluster_idx)\n",
    "    #print(\"Reading complete\")\n",
    "\n",
    "    # Decompression GPU\n",
    "    #print(\"\\nGPU decompression\")\n",
    "    codec = NvCompBatchCodec(\"zstd\")\n",
    "    codec.decode_batch(all_compressed_buffers, all_output_buffers)\n",
    "    #print(\"GPU decompression complete\")\n",
    "\n",
    "    #print(\"Process decompressed data\")\n",
    "    content_dict = Process_decompressed_content(in_ntuple,\n",
    "                                          target_cols,\n",
    "                                          start_cluster_idx,\n",
    "                                          stop_cluster_idx,\n",
    "                                          all_cluster_output_buffers)\n",
    "    #if prototype:\n",
    "    #    return(content_dict, form)\n",
    "\n",
    "    # Pick back up here\n",
    "    container_dict = {}\n",
    "\n",
    "\n",
    "    # return(ak.unflatten(content, cp.diff(offsets))) \n",
    "    \n",
    "    # Debugging\n",
    "    for key in target_cols:\n",
    "        if \"column\" in key and \"union\" not in key:\n",
    "            key_nr = int(key.split(\"-\")[1])\n",
    "            dtype_byte = in_ntuple.ntuple.column_records[key_nr].type\n",
    "            content = content_dict[key_nr].view(dtype = 'b')\n",
    "\n",
    "            if \"cardinality\" in key:\n",
    "                content = numpy.diff(content)\n",
    "            if dtype_byte == uproot.const.rntuple_col_type_to_num_dict[\"switch\"]:\n",
    "                kindex, tags = _split_switch_bits(content)\n",
    "                # Find invalid variants and adjust buffers accordingly\n",
    "                invalid = numpy.flatnonzero(tags == -1)\n",
    "                if len(invalid) > 0:\n",
    "                    kindex = numpy.delete(kindex, invalid)\n",
    "                    tags = numpy.delete(tags, invalid)\n",
    "                    invalid -= numpy.arange(len(invalid))\n",
    "                    optional_index = numpy.insert(\n",
    "                        numpy.arange(len(kindex), dtype=numpy.int64), invalid, -1\n",
    "                    )\n",
    "                else:\n",
    "                    optional_index = numpy.arange(len(kindex), dtype=numpy.int64)\n",
    "                container_dict[f\"{key}-index\"] = optional_index\n",
    "                container_dict[f\"{key}-union-index\"] = kindex\n",
    "                container_dict[f\"{key}-union-tags\"] = tags\n",
    "            else:\n",
    "                # don't distinguish data and offsets\n",
    "                container_dict[f\"{key}-data\"] = content\n",
    "                container_dict[f\"{key}-offsets\"] = content\n",
    "    cluster_offset = cluster_starts[start_cluster_idx]\n",
    "    entry_start -= cluster_offset\n",
    "    entry_stop -= cluster_offset\n",
    "\n",
    "    return ak.from_buffers(\n",
    "        form, cluster_num_entries, container_dict, allow_noncanonical_form=True,\n",
    "        backend = \"cuda\"\n",
    "    )[entry_start:entry_stop]\n",
    "\n",
    "def GPU_akzip_RNTuple(container_dict, form):\n",
    "    # Associate content/offsets with their keys\n",
    "    arrays = {}\n",
    "\n",
    "    form_contents = form.contents\n",
    "    for i, field in enumerate(form.fields):\n",
    "        field_content = form_contents[i]\n",
    "\n",
    "        # if field_content is instance(ak.forms.NumpyForm):\n",
    "        \n",
    "        if isinstance(field_content, ak.forms.ListOffsetForm):\n",
    "            offsets_key = int(field_content.form_key.split(\"-\")[1])\n",
    "            content_key = int(field_content.content.form_key.split(\"-\")[1])\n",
    "            array_ = ak.unflatten(container_dict[content_key], cp.diff(container_dict[offsets_key]))\n",
    "            arrays[field] = array_\n",
    "        elif isinstance(field_content, ak.forms.NumpyForm):\n",
    "            content_key = int(field_content.form_key.split(\"-\")[1])\n",
    "            arrays[field] = ak.Array(container_dict[content_key])\n",
    "    \n",
    "    return(ak.zip(arrays, depth_limit = 1))\n",
    "\n",
    "def kvikUproot_openGPU(in_ntuple_path, columns, classname):\n",
    "    events_RNTuple_GPU = kvikuproot_open_RNTuple(in_ntuple_path, columns, classname)\n",
    "    # events_RNTuple_GPU = GPU_akzip_RNTuple(content_dict, form)\n",
    "\n",
    "    return(events_RNTuple_GPU)\n",
    "\n",
    "path_rntuple = \"/home/fstrug/uscmshome/nobackup/GPU/kvikio_playground/TTToSemiLeptonic_UL18JMERNTuple-zstd.root\"\n",
    "cols = [\"Electron_pt\", \"Electron_eta\", \"Electron_phi\",\n",
    "            \"Muon_pt\", \"Muon_eta\", \"Muon_phi\",\n",
    "            \"FatJet_pt\", \"FatJet_eta\", \"FatJet_phi\",\n",
    "            \"Jet_pt\", \"Jet_eta\", \"Jet_phi\",\n",
    "            \"MET_pt\"]\n",
    "classname = \"Events\"\n",
    "\n",
    "events_RNTuple_GPU = kvikUproot_openGPU(path_rntuple, cols, classname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "400cb4aa-6069-41bc-adee-dfe54e40d43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify result with Uproot implementation\n",
    "import uproot\n",
    "path_rntuple = \"/home/fstrug/uscmshome/nobackup/GPU/kvikio_playground/TTToSemiLeptonic_UL18JMERNTuple-zstd.root\"\n",
    "cols = [\"Electron_pt\", \"Electron_eta\", \"Electron_phi\",\n",
    "            \"Muon_pt\", \"Muon_eta\", \"Muon_phi\",\n",
    "            \"FatJet_pt\", \"FatJet_eta\", \"FatJet_phi\",\n",
    "            \"Jet_pt\", \"Jet_eta\", \"Jet_phi\",\n",
    "            \"MET_pt\"]\n",
    "classname = \"Events\"\n",
    "\n",
    "f = uproot.open(path_rntuple)\n",
    "rntuple = f[\"Events\"]\n",
    "events_RNTuple_CPU = rntuple.arrays(cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fadebdb9-5abc-4a29-817a-0b98cdb5f1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "events_RNTuple_CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6698545c-c315-4dcc-ac9e-25f483548ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in cols:\n",
    "    assert ak.all(events_RNTuple_GPU[col] == ak.to_backend(events_RNTuple_CPU[col], \"cuda\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7386e9d6-2570-491a-a3a1-0b5fe89f00d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy as cp\n",
    "array = cp.array([1,2,3,4])\n",
    "array_ptr1 = array[0:2]\n",
    "array_ptr2 = array[2:4]\n",
    "print(\"Base array: \\t\\t\", id(array), array)\n",
    "print(\"\")\n",
    "print(\"Sub array1: \\t\\t\", id(array_ptr1) , array_ptr1)\n",
    "print(\"Sub array1 base: \\t\", id(array_ptr1.base), array_ptr1.base)\n",
    "print(\"\")\n",
    "print(\"Sub array2: \\t\\t\", id(array_ptr2) , array_ptr2)\n",
    "print(\"Sub array2 base: \\t\", id(array_ptr2.base), array_ptr2.base)\n",
    "\n",
    "if id(array) == id(array_ptr1.base) and id(array) == id(array_ptr2.base):\n",
    "    print(\"Base array and sub arrays base id match\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65834513-c228-4a13-9440-a0e476309160",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using ndarray.data\n",
    "print(\"Base array data: \\t\", array.data)\n",
    "print(\"Sub array1 data: \\t\", array_ptr1.data)\n",
    "print(\"Sub array2 data: \\t\", array_ptr2.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f604e5-8f8a-42ef-bd49-57b1d72c150d",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(array.view(\"b\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-kvikio-cuda12.5-py3.11]",
   "language": "python",
   "name": "conda-env-.conda-kvikio-cuda12.5-py3.11-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
