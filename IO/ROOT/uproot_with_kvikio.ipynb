{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "130d935e-6964-4bfa-98ff-52aff8d13071",
   "metadata": {},
   "source": [
    "# Nick's Code - CPU Decompression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7d69af-7f2b-4b11-a373-dd1b2dbb362c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uproot\n",
    "import cramjam\n",
    "import numpy as np\n",
    "import awkward as ak\n",
    "\n",
    "def basket_to_array(data: bytes, dtype: np.dtype, expected_entries: int):\n",
    "    format1 = uproot.models.TBasket._tbasket_format1\n",
    "    (\n",
    "        fNbytes,\n",
    "        key_version,\n",
    "        fObjlen,\n",
    "        fDatime,\n",
    "        fKeylen,\n",
    "        fCycle,\n",
    "    ) = format1.unpack(data[: format1.size])\n",
    "    assert fNbytes == len(data)\n",
    "    assert fNbytes - fKeylen != fObjlen  # this is only true for uncompressed baskets\n",
    "\n",
    "    format2 = uproot.models.TBasket._tbasket_format2\n",
    "    (\n",
    "        fVersion,\n",
    "        fBufferSize,\n",
    "        fNevBufSize,\n",
    "        fNevBuf,\n",
    "        fLast,\n",
    "    ) = format2.unpack(data[fKeylen - format2.size - 1: fKeylen - 1])\n",
    "    border = fLast - fKeylen  # border between the contents and offsets in uncompressed data\n",
    "    assert border <= fObjlen, f\"{border} <= {fObjlen}\"\n",
    "    assert expected_entries == border // dtype.itemsize\n",
    "\n",
    "    format3 = uproot.compression._decompress_header_format\n",
    "    algo, method, c1, c2, c3, u1, u2, u3 = format3.unpack(\n",
    "        data[fKeylen : fKeylen + format3.size]\n",
    "    )\n",
    "    assert algo == b\"ZS\", f\"Unsupported algorithm: {algo}\"  # zstd, we can support more later\n",
    "    block_compressed_bytes = c1 + (c2 << 8) + (c3 << 16)\n",
    "    block_uncompressed_bytes = u1 + (u2 << 8) + (u3 << 16)\n",
    "    assert fObjlen == block_uncompressed_bytes\n",
    "    assert len(data) == fKeylen + format3.size + block_compressed_bytes  # may not be true for baskets larger than 16 MiB\n",
    "    compressed_content = data[fKeylen + format3.size :]    \n",
    "    \n",
    "    raw_content = cramjam.zstd.decompress(compressed_content, output_len=block_uncompressed_bytes)\n",
    "    content = np.frombuffer(\n",
    "    raw_content, dtype=dtype, count=border // dtype.itemsize\n",
    "    )\n",
    "    return content\n",
    "\n",
    "\n",
    "def all_baskets_to_array(filehandle, branch):\n",
    "    basket_starts = branch.all_members[\"fBasketSeek\"]\n",
    "    basket_lengths = branch.all_members[\"fBasketBytes\"]\n",
    "    basket_entryoffsets = branch.all_members[\"fBasketEntry\"]\n",
    "    assert basket_starts[-1] == 0  # offsets array is one larger than the number of baskets\n",
    "    assert basket_lengths[-1] == 0\n",
    "\n",
    "    if isinstance(branch.interpretation, uproot.interpretation.jagged.AsJagged):\n",
    "        counts = all_baskets_to_array(filehandle, branch.count_branch)\n",
    "        assert len(counts) == basket_entryoffsets[-1]\n",
    "        offsets = np.zeros(len(counts) + 1, dtype=np.int64)\n",
    "        offsets[1:] = np.cumsum(counts)\n",
    "        dtype = branch.interpretation.content.from_dtype\n",
    "        content = np.empty(offsets[-1], dtype=dtype)\n",
    "        for i in range(len(basket_starts) - 1):\n",
    "            filehandle.seek(basket_starts[i])\n",
    "            data = filehandle.read(basket_lengths[i])\n",
    "            # print(f\"Reading basket {i} with {len(data)} bytes at entry offset {basket_entryoffsets[i]}:{basket_entryoffsets[i+1]}\")\n",
    "            put_start = offsets[basket_entryoffsets[i]]\n",
    "            put_stop = offsets[basket_entryoffsets[i+1]]\n",
    "            # print(f\"Destination: {put_start}:{put_stop}\")\n",
    "            basket_content = basket_to_array(data, dtype, put_stop - put_start)\n",
    "            content[put_start:put_stop] = basket_content\n",
    "        return ak.unflatten(content.astype(branch.interpretation.content.to_dtype), counts)\n",
    "    elif isinstance(branch.interpretation, uproot.interpretation.numerical.AsDtype):\n",
    "        dtype = branch.interpretation.from_dtype\n",
    "        content = np.empty(basket_entryoffsets[-1], dtype=dtype)\n",
    "        for i in range(len(basket_starts) - 1):\n",
    "            filehandle.seek(basket_starts[i])\n",
    "            data = filehandle.read(basket_lengths[i])\n",
    "            # print(f\"Reading basket {i} with {len(data)} bytes at entry offset {basket_entryoffsets[i]}:{basket_entryoffsets[i+1]}\")\n",
    "            basket_content = basket_to_array(data, dtype, basket_entryoffsets[i+1] - basket_entryoffsets[i])\n",
    "            # would be better to pass the content buffer into basket_to_array and decompress directly into it\n",
    "            content[basket_entryoffsets[i]:basket_entryoffsets[i+1]] = basket_content\n",
    "        return content.astype(branch.interpretation.to_dtype)\n",
    "    raise NotImplementedError(\"Only AsJagged and AsDtype are supported\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e347bc-e595-4850-b7ea-02e9df0aa69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "# /store/user/IDAP/RunIISummer20UL18NanoAODv9/TTToSemiLeptonic_TuneCP5_13TeV-powheg-pythia8/NANOAODSIM/20UL18JMENano_106X_upgrade2018_realistic_v16_L1v1-v1/40000/BCB3E2FC-D575-0341-A211-5C9A8D8798B9.root\n",
    "filename = \"/home/fstrug/uscmshome/nobackup/GPU/kvikio_playground/TTToSemiLeptonic_UL18JMENanoAOD-zstd.root\"\n",
    "file = uproot.open(filename)\n",
    "tree = file[\"Events\"]\n",
    "branch = tree[\"Muon_pt\"]\n",
    "\n",
    "with open(filename, \"rb\") as rawfile:\n",
    "    myarray = all_baskets_to_array(rawfile, branch)\n",
    "\n",
    "assert ak.all(myarray == branch.array(library=\"ak\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07adab8a-e4e1-4120-b5f8-bf5d60e3f2d3",
   "metadata": {},
   "source": [
    "# GPU Decompression (SLOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f9e410-be60-4cac-a167-75080ef51ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uproot\n",
    "import cramjam\n",
    "import numpy as np\n",
    "import awkward as ak\n",
    "from kvikio.nvcomp_codec import NvCompBatchCodec\n",
    "\n",
    "def GPU_basket_to_array(data: bytes, dtype: np.dtype, expected_entries: int):\n",
    "    format1 = uproot.models.TBasket._tbasket_format1\n",
    "    (\n",
    "        fNbytes,\n",
    "        key_version,\n",
    "        fObjlen,\n",
    "        fDatime,\n",
    "        fKeylen,\n",
    "        fCycle,\n",
    "    ) = format1.unpack(data[: format1.size])\n",
    "    assert fNbytes == len(data)\n",
    "    assert fNbytes - fKeylen != fObjlen  # this is only true for uncompressed baskets\n",
    "\n",
    "    format2 = uproot.models.TBasket._tbasket_format2\n",
    "    (\n",
    "        fVersion,\n",
    "        fBufferSize,\n",
    "        fNevBufSize,\n",
    "        fNevBuf,\n",
    "        fLast,\n",
    "    ) = format2.unpack(data[fKeylen - format2.size - 1: fKeylen - 1])\n",
    "    border = fLast - fKeylen  # border between the contents and offsets in uncompressed data\n",
    "    assert border <= fObjlen, f\"{border} <= {fObjlen}\"\n",
    "    assert expected_entries == border // dtype.itemsize\n",
    "\n",
    "    format3 = uproot.compression._decompress_header_format\n",
    "    algo, method, c1, c2, c3, u1, u2, u3 = format3.unpack(\n",
    "        data[fKeylen : fKeylen + format3.size]\n",
    "    )\n",
    "    assert algo == b\"ZS\", f\"Unsupported algorithm: {algo}\"  # zstd, we can support more later\n",
    "    block_compressed_bytes = c1 + (c2 << 8) + (c3 << 16)\n",
    "    block_uncompressed_bytes = u1 + (u2 << 8) + (u3 << 16)\n",
    "    assert fObjlen == block_uncompressed_bytes\n",
    "    assert len(data) == fKeylen + format3.size + block_compressed_bytes  # may not be true for baskets larger than 16 MiB\n",
    "    compressed_content = data[fKeylen + format3.size :]\n",
    "\n",
    "    #############################\n",
    "    #Decompress on CPU\n",
    "    raw_content = cramjam.zstd.decompress(compressed_content, output_len=block_uncompressed_bytes)\n",
    "    content = np.frombuffer(\n",
    "    raw_content, dtype=dtype, count=border // dtype.itemsize\n",
    "    )\n",
    "    \n",
    "    \n",
    "    #############################\n",
    "    # Decompress on GPU\n",
    "    from kvikio.nvcomp_codec import NvCompBatchCodec\n",
    "    codec = NvCompBatchCodec(\"zstd\")\n",
    "    content = codec.decode(compressed_content).view(dtype=dtype)[:border // dtype.itemsize]\n",
    "    \n",
    "    \n",
    "    \n",
    "    return content\n",
    "\n",
    "\n",
    "def GPU_all_baskets_to_array(filehandle, branch):\n",
    "    basket_starts = branch.all_members[\"fBasketSeek\"]\n",
    "    basket_lengths = branch.all_members[\"fBasketBytes\"]\n",
    "    basket_entryoffsets = branch.all_members[\"fBasketEntry\"]\n",
    "    assert basket_starts[-1] == 0  # offsets array is one larger than the number of baskets\n",
    "    assert basket_lengths[-1] == 0\n",
    "\n",
    "    if isinstance(branch.interpretation, uproot.interpretation.jagged.AsJagged):\n",
    "        counts = GPU_all_baskets_to_array(filehandle, branch.count_branch)\n",
    "        assert len(counts) == basket_entryoffsets[-1]\n",
    "        offsets = np.zeros(len(counts) + 1, dtype=np.int64)\n",
    "        offsets[1:] = np.cumsum(counts)\n",
    "        dtype = branch.interpretation.content.from_dtype\n",
    "        content = np.empty(offsets[-1], dtype=dtype)\n",
    "        for i in range(len(basket_starts) - 1):\n",
    "            filehandle.seek(basket_starts[i])\n",
    "            data = filehandle.read(basket_lengths[i])\n",
    "            # print(f\"Reading basket {i} with {len(data)} bytes at entry offset {basket_entryoffsets[i]}:{basket_entryoffsets[i+1]}\")\n",
    "            put_start = offsets[basket_entryoffsets[i]]\n",
    "            put_stop = offsets[basket_entryoffsets[i+1]]\n",
    "            # print(f\"Destination: {put_start}:{put_stop}\")\n",
    "            basket_content = GPU_basket_to_array(data, dtype, put_stop - put_start)\n",
    "            content[put_start:put_stop] = basket_content\n",
    "        return ak.unflatten(content.astype(branch.interpretation.content.to_dtype), counts)\n",
    "        \n",
    "    elif isinstance(branch.interpretation, uproot.interpretation.numerical.AsDtype):\n",
    "        dtype = branch.interpretation.from_dtype\n",
    "        content = np.empty(basket_entryoffsets[-1], dtype=dtype)\n",
    "        for i in range(len(basket_starts) - 1):\n",
    "            filehandle.seek(basket_starts[i])\n",
    "            data = filehandle.read(basket_lengths[i])\n",
    "            # print(f\"Reading basket {i} with {len(data)} bytes at entry offset {basket_entryoffsets[i]}:{basket_entryoffsets[i+1]}\")\n",
    "            basket_content = GPU_basket_to_array(data, dtype, basket_entryoffsets[i+1] - basket_entryoffsets[i])\n",
    "            # would be better to pass the content buffer into GPU_basket_to_array and decompress directly into it\n",
    "            content[basket_entryoffsets[i]:basket_entryoffsets[i+1]] = basket_content\n",
    "        return content.astype(branch.interpretation.to_dtype)\n",
    "    raise NotImplementedError(\"Only AsJagged and AsDtype are supported\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ea3d5d-5deb-458b-9b37-c059b7b4deab",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "# /store/user/IDAP/RunIISummer20UL18NanoAODv9/TTToSemiLeptonic_TuneCP5_13TeV-powheg-pythia8/NANOAODSIM/20UL18JMENano_106X_upgrade2018_realistic_v16_L1v1-v1/40000/BCB3E2FC-D575-0341-A211-5C9A8D8798B9.root\n",
    "filename = \"/home/fstrug/uscmshome/nobackup/GPU/kvikio_playground/TTToSemiLeptonic_UL18JMENanoAOD-zstd.root\"\n",
    "file = uproot.open(filename)\n",
    "tree = file[\"Events\"]\n",
    "branch = tree[\"Muon_pt\"]\n",
    "\n",
    "with open(filename, \"rb\") as rawfile:\n",
    "    myarray = GPU_all_baskets_to_array(rawfile, branch)\n",
    "\n",
    "assert ak.all(myarray == branch.array(library=\"ak\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd97d2c-e7ac-4cfc-b389-27d3d0097173",
   "metadata": {},
   "source": [
    "# GPU (FAST-er)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a286dc-159a-4cdc-b13f-2755bbdb28d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uproot\n",
    "import cramjam\n",
    "import numpy as np\n",
    "import awkward as ak\n",
    "from kvikio.nvcomp_codec import NvCompBatchCodec\n",
    "\n",
    "def GPU_basket_to_array(data: bytes, dtype: np.dtype, expected_entries: int):\n",
    "    format1 = uproot.models.TBasket._tbasket_format1\n",
    "    (\n",
    "        fNbytes,\n",
    "        key_version,\n",
    "        fObjlen,\n",
    "        fDatime,\n",
    "        fKeylen,\n",
    "        fCycle,\n",
    "    ) = format1.unpack(data[: format1.size])\n",
    "    assert fNbytes == len(data)\n",
    "    assert fNbytes - fKeylen != fObjlen  # this is only true for uncompressed baskets\n",
    "\n",
    "    format2 = uproot.models.TBasket._tbasket_format2\n",
    "    (\n",
    "        fVersion,\n",
    "        fBufferSize,\n",
    "        fNevBufSize,\n",
    "        fNevBuf,\n",
    "        fLast,\n",
    "    ) = format2.unpack(data[fKeylen - format2.size - 1: fKeylen - 1])\n",
    "    border = fLast - fKeylen  # border between the contents and offsets in uncompressed data\n",
    "    assert border <= fObjlen, f\"{border} <= {fObjlen}\"\n",
    "    assert expected_entries == border // dtype.itemsize\n",
    "\n",
    "    format3 = uproot.compression._decompress_header_format\n",
    "    algo, method, c1, c2, c3, u1, u2, u3 = format3.unpack(\n",
    "        data[fKeylen : fKeylen + format3.size]\n",
    "    )\n",
    "    assert algo == b\"ZS\", f\"Unsupported algorithm: {algo}\"  # zstd, we can support more later\n",
    "    block_compressed_bytes = c1 + (c2 << 8) + (c3 << 16)\n",
    "    block_uncompressed_bytes = u1 + (u2 << 8) + (u3 << 16)\n",
    "    assert fObjlen == block_uncompressed_bytes\n",
    "    assert len(data) == fKeylen + format3.size + block_compressed_bytes  # may not be true for baskets larger than 16 MiB\n",
    "    compressed_content = data[fKeylen + format3.size :]\n",
    "\n",
    "    #############################\n",
    "    #Decompress on CPU\n",
    "    # raw_content = cramjam.zstd.decompress(compressed_content, output_len=block_uncompressed_bytes)\n",
    "    # content = np.frombuffer(\n",
    "    # raw_content, dtype=dtype, count=border // dtype.itemsize\n",
    "    # )\n",
    "    \n",
    "    return (compressed_content, border)\n",
    "\n",
    "def GPU_all_baskets_to_array(filehandle, branch):\n",
    "    basket_starts = branch.all_members[\"fBasketSeek\"]\n",
    "    basket_lengths = branch.all_members[\"fBasketBytes\"]\n",
    "    basket_entryoffsets = branch.all_members[\"fBasketEntry\"]\n",
    "    assert basket_starts[-1] == 0  # offsets array is one larger than the number of baskets\n",
    "    assert basket_lengths[-1] == 0\n",
    "\n",
    "    if isinstance(branch.interpretation, uproot.interpretation.jagged.AsJagged):\n",
    "        counts = GPU_all_baskets_to_array(filehandle, branch.count_branch)\n",
    "        assert len(counts) == basket_entryoffsets[-1]\n",
    "        offsets = np.zeros(len(counts) + 1, dtype=np.int64)\n",
    "        offsets[1:] = np.cumsum(counts)\n",
    "        dtype = branch.interpretation.content.from_dtype\n",
    "        content = np.empty(offsets[-1], dtype=dtype)\n",
    "\n",
    "        #####\n",
    "        N_baskets = len(basket_starts) - 1\n",
    "        compressed_contents = []\n",
    "        borders = []\n",
    "        put_starts = []\n",
    "        put_stops = []\n",
    "\n",
    "        \n",
    "        for i in range(N_baskets):\n",
    "            filehandle.seek(basket_starts[i])\n",
    "            data = filehandle.read(basket_lengths[i])\n",
    "            # print(f\"Reading basket {i} with {len(data)} bytes at entry offset {basket_entryoffsets[i]}:{basket_entryoffsets[i+1]}\")\n",
    "            put_starts.append(offsets[basket_entryoffsets[i]])\n",
    "            put_stops.append(offsets[basket_entryoffsets[i+1]])\n",
    "            # print(f\"Destination: {put_start}:{put_stop}\")\n",
    "            compressed_content, border = GPU_basket_to_array(data, dtype, put_stops[i] - put_starts[i])\n",
    "            compressed_contents.append(compressed_content)\n",
    "            borders.append(border)\n",
    "            \n",
    "        #Decompress batch of compressed_content    \n",
    "        codec = NvCompBatchCodec(\"zstd\")\n",
    "        decompressed_contents = codec.decode_batch(compressed_contents)\n",
    "\n",
    "        for i in range(N_baskets):\n",
    "            content[put_starts[i]:put_stops[i]] = decompressed_contents[i].view(dtype)[:borders[i] // dtype.itemsize]\n",
    "        \n",
    "        return ak.unflatten(content.astype(branch.interpretation.content.to_dtype), counts)\n",
    "        \n",
    "    elif isinstance(branch.interpretation, uproot.interpretation.numerical.AsDtype):\n",
    "        dtype = branch.interpretation.from_dtype\n",
    "        content = np.empty(basket_entryoffsets[-1], dtype=dtype)\n",
    "\n",
    "        #####\n",
    "        N_baskets = len(basket_starts) - 1\n",
    "        compressed_contents = []\n",
    "        borders = []\n",
    "        put_starts = []\n",
    "        put_stops = []\n",
    "        \n",
    "        \n",
    "        for i in range(N_baskets):\n",
    "            filehandle.seek(basket_starts[i])\n",
    "            data = filehandle.read(basket_lengths[i])\n",
    "\n",
    "            put_starts.append(basket_entryoffsets[i])\n",
    "            put_stops.append(basket_entryoffsets[i+1])\n",
    "            # print(f\"Reading basket {i} with {len(data)} bytes at entry offset {basket_entryoffsets[i]}:{basket_entryoffsets[i+1]}\")\n",
    "            compressed_content, border = GPU_basket_to_array(data, dtype, put_stops[i] - put_starts[i])\n",
    "            compressed_contents.append(compressed_content)\n",
    "            borders.append(border)\n",
    "            \n",
    "        #Decompress batch of compressed_content    \n",
    "        codec = NvCompBatchCodec(\"zstd\")\n",
    "        decompressed_contents = codec.decode_batch(compressed_contents)\n",
    "\n",
    "        for i in range(N_baskets):\n",
    "            content[put_starts[i]:put_stops[i]] = decompressed_contents[i].view(dtype)[:borders[i] // dtype.itemsize]\n",
    "\n",
    "        return content.astype(branch.interpretation.to_dtype)\n",
    "    raise NotImplementedError(\"Only AsJagged and AsDtype are supported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b9c46b-e946-458a-ba84-56a24528bc71",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "# /store/user/IDAP/RunIISummer20UL18NanoAODv9/TTToSemiLeptonic_TuneCP5_13TeV-powheg-pythia8/NANOAODSIM/20UL18JMENano_106X_upgrade2018_realistic_v16_L1v1-v1/40000/BCB3E2FC-D575-0341-A211-5C9A8D8798B9.root\n",
    "filename = \"/home/fstrug/uscmshome/nobackup/GPU/kvikio_playground/TTToSemiLeptonic_UL18JMENanoAOD-zstd.root\"\n",
    "file = uproot.open(filename)\n",
    "tree = file[\"Events\"]\n",
    "branch = tree[\"Muon_pt\"]\n",
    "\n",
    "with open(filename, \"rb\") as rawfile:\n",
    "    myarray = GPU_all_baskets_to_array(rawfile, branch)\n",
    "\n",
    "assert ak.all(myarray == branch.array(library=\"ak\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cddb37c6-6ebc-458b-8a7d-905646fb9e41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-kvikio-env]",
   "language": "python",
   "name": "conda-env-.conda-kvikio-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
