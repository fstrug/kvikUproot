{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4435b0d5-61e5-4644-aa2b-18d79b070aff",
   "metadata": {},
   "source": [
    "# Uproot - GPU Decompression of Multiple Branches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e574d18d-6437-4947-8ce1-90cf8d143b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uproot\n",
    "import numpy as np\n",
    "import awkward as ak\n",
    "from kvikio.nvcomp_codec import NvCompBatchCodec\n",
    "import cramjam\n",
    "\n",
    "def basket_to_compcont_border(data: bytes, dtype: np.dtype, expected_entries: int):\n",
    "    format1 = uproot.models.TBasket._tbasket_format1\n",
    "    (\n",
    "        fNbytes,\n",
    "        key_version,\n",
    "        fObjlen,\n",
    "        fDatime,\n",
    "        fKeylen,\n",
    "        fCycle,\n",
    "    ) = format1.unpack(data[: format1.size])\n",
    "    assert fNbytes == len(data)\n",
    "    assert fNbytes - fKeylen != fObjlen  # this is only true for uncompressed baskets\n",
    "\n",
    "    format2 = uproot.models.TBasket._tbasket_format2\n",
    "    (\n",
    "        fVersion,\n",
    "        fBufferSize,\n",
    "        fNevBufSize,\n",
    "        fNevBuf,\n",
    "        fLast,\n",
    "    ) = format2.unpack(data[fKeylen - format2.size - 1: fKeylen - 1])\n",
    "    border = fLast - fKeylen  # border between the contents and offsets in uncompressed data\n",
    "    assert border <= fObjlen, f\"{border} <= {fObjlen}\"\n",
    "    assert expected_entries == border // dtype.itemsize\n",
    "\n",
    "    format3 = uproot.compression._decompress_header_format\n",
    "    algo, method, c1, c2, c3, u1, u2, u3 = format3.unpack(\n",
    "        data[fKeylen : fKeylen + format3.size]\n",
    "    )\n",
    "    assert algo == b\"ZS\", f\"Unsupported algorithm: {algo}\"  # zstd, we can support more later\n",
    "    block_compressed_bytes = c1 + (c2 << 8) + (c3 << 16)\n",
    "    block_uncompressed_bytes = u1 + (u2 << 8) + (u3 << 16)\n",
    "    assert fObjlen == block_uncompressed_bytes\n",
    "    assert len(data) == fKeylen + format3.size + block_compressed_bytes  # may not be true for baskets larger than 16 MiB\n",
    "    compressed_content = data[fKeylen + format3.size :]\n",
    "    \n",
    "    return (compressed_content, border)\n",
    "\n",
    "def basket_to_array(data: bytes, dtype: np.dtype, expected_entries: int):\n",
    "    format1 = uproot.models.TBasket._tbasket_format1\n",
    "    (\n",
    "        fNbytes,\n",
    "        key_version,\n",
    "        fObjlen,\n",
    "        fDatime,\n",
    "        fKeylen,\n",
    "        fCycle,\n",
    "    ) = format1.unpack(data[: format1.size])\n",
    "    assert fNbytes == len(data)\n",
    "    assert fNbytes - fKeylen != fObjlen  # this is only true for uncompressed baskets\n",
    "\n",
    "    format2 = uproot.models.TBasket._tbasket_format2\n",
    "    (\n",
    "        fVersion,\n",
    "        fBufferSize,\n",
    "        fNevBufSize,\n",
    "        fNevBuf,\n",
    "        fLast,\n",
    "    ) = format2.unpack(data[fKeylen - format2.size - 1: fKeylen - 1])\n",
    "    border = fLast - fKeylen  # border between the contents and offsets in uncompressed data\n",
    "    assert border <= fObjlen, f\"{border} <= {fObjlen}\"\n",
    "    assert expected_entries == border // dtype.itemsize\n",
    "\n",
    "    format3 = uproot.compression._decompress_header_format\n",
    "    algo, method, c1, c2, c3, u1, u2, u3 = format3.unpack(\n",
    "        data[fKeylen : fKeylen + format3.size]\n",
    "    )\n",
    "    assert algo == b\"ZS\", f\"Unsupported algorithm: {algo}\"  # zstd, we can support more later\n",
    "    block_compressed_bytes = c1 + (c2 << 8) + (c3 << 16)\n",
    "    block_uncompressed_bytes = u1 + (u2 << 8) + (u3 << 16)\n",
    "    assert fObjlen == block_uncompressed_bytes\n",
    "    assert len(data) == fKeylen + format3.size + block_compressed_bytes  # may not be true for baskets larger than 16 MiB\n",
    "    compressed_content = data[fKeylen + format3.size :]    \n",
    "    \n",
    "    raw_content = cramjam.zstd.decompress(compressed_content, output_len=block_uncompressed_bytes)\n",
    "    content = np.frombuffer(\n",
    "    raw_content, dtype=dtype, count=border // dtype.itemsize\n",
    "    )\n",
    "    return content\n",
    "\n",
    "def get_counts(filehandle, branch):\n",
    "    basket_starts = branch.all_members[\"fBasketSeek\"]\n",
    "    basket_lengths = branch.all_members[\"fBasketBytes\"]\n",
    "    basket_entryoffsets = branch.all_members[\"fBasketEntry\"]\n",
    "    assert basket_starts[-1] == 0  # offsets array is one larger than the number of baskets\n",
    "    assert basket_lengths[-1] == 0\n",
    "    dtype = branch.interpretation.from_dtype\n",
    "    content = np.empty(basket_entryoffsets[-1], dtype=dtype)\n",
    "    for i in range(len(basket_starts) - 1):\n",
    "        filehandle.seek(basket_starts[i])\n",
    "        data = filehandle.read(basket_lengths[i])\n",
    "        # print(f\"Reading basket {i} with {len(data)} bytes at entry offset {basket_entryoffsets[i]}:{basket_entryoffsets[i+1]}\")\n",
    "        basket_content = basket_to_array(data, dtype, basket_entryoffsets[i+1] - basket_entryoffsets[i])\n",
    "        # would be better to pass the content buffer into GPU_basket_to_array and decompress directly into it\n",
    "        content[basket_entryoffsets[i]:basket_entryoffsets[i+1]] = basket_content\n",
    "    return content.astype(branch.interpretation.to_dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7578c38-b457-4da0-990e-2ffde12726d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GPU_all_baskets_to_array(filehandle, branches):\n",
    "    arrays = {}\n",
    "    branches_metadatas = {}\n",
    "    for branch in branches:\n",
    "        if isinstance(branch.interpretation, uproot.interpretation.jagged.AsJagged):\n",
    "            basket_starts = branch.all_members[\"fBasketSeek\"]\n",
    "            basket_lengths = branch.all_members[\"fBasketBytes\"]\n",
    "            basket_entryoffsets = branch.all_members[\"fBasketEntry\"]\n",
    "            assert basket_starts[-1] == 0  # offsets array is one larger than the number of baskets\n",
    "            assert basket_lengths[-1] == 0\n",
    "            \n",
    "            counts = get_counts(filehandle, branch.count_branch)\n",
    "            assert len(counts) == basket_entryoffsets[-1]\n",
    "            offsets = np.zeros(len(counts) + 1, dtype=np.int64)\n",
    "            offsets[1:] = np.cumsum(counts)\n",
    "            dtype = branch.interpretation.content.from_dtype\n",
    "            content = np.empty(offsets[-1], dtype=dtype)\n",
    "    \n",
    "            #####\n",
    "            # Baskets' metadata\n",
    "            N_baskets = len(basket_starts) - 1\n",
    "            compressed_contents = []\n",
    "            borders = []\n",
    "            put_starts = []\n",
    "            put_stops = []\n",
    "    \n",
    "            \n",
    "            for i in range(N_baskets):\n",
    "                # Grab each basket's metadata and store for later\n",
    "                filehandle.seek(basket_starts[i])\n",
    "                data = filehandle.read(basket_lengths[i])\n",
    "                # print(f\"Reading basket {i} with {len(data)} bytes at entry offset {basket_entryoffsets[i]}:{basket_entryoffsets[i+1]}\")\n",
    "                put_starts.append(offsets[basket_entryoffsets[i]])\n",
    "                put_stops.append(offsets[basket_entryoffsets[i+1]])\n",
    "                # print(f\"Destination: {put_start}:{put_stop}\")\n",
    "                compressed_content, border = basket_to_compcont_border(data, dtype, put_stops[i] - put_starts[i])\n",
    "                compressed_contents.append(compressed_content)\n",
    "                borders.append(border)\n",
    "            \n",
    "            branch_metadata = {}\n",
    "            branch_metadata[\"N_baskets\"] = N_baskets\n",
    "            branch_metadata[\"counts\"] = counts\n",
    "            branch_metadata[\"content\"] = content\n",
    "            branch_metadata[\"compressed_contents\"] = compressed_contents\n",
    "            branch_metadata[\"borders\"] = borders\n",
    "            branch_metadata[\"put_starts\"] = put_starts\n",
    "            branch_metadata[\"put_stops\"] = put_stops\n",
    "            branches_metadatas[branch.name] = branch_metadata\n",
    "            \n",
    "        elif isinstance(branch.interpretation, uproot.interpretation.numerical.AsDtype):\n",
    "            basket_starts = branch.all_members[\"fBasketSeek\"]\n",
    "            basket_lengths = branch.all_members[\"fBasketBytes\"]\n",
    "            basket_entryoffsets = branch.all_members[\"fBasketEntry\"]\n",
    "            assert basket_starts[-1] == 0  # offsets array is one larger than the number of baskets\n",
    "            assert basket_lengths[-1] == 0\n",
    "            \n",
    "            dtype = branch.interpretation.from_dtype\n",
    "            content = np.empty(basket_entryoffsets[-1], dtype=dtype)\n",
    "    \n",
    "            #####\n",
    "            N_baskets = len(basket_starts) - 1\n",
    "            compressed_contents = []\n",
    "            borders = []\n",
    "            put_starts = []\n",
    "            put_stops = []\n",
    "            \n",
    "            for i in range(N_baskets):\n",
    "                filehandle.seek(basket_starts[i])\n",
    "                data = filehandle.read(basket_lengths[i])\n",
    "                put_starts.append(basket_entryoffsets[i])\n",
    "                put_stops.append(basket_entryoffsets[i+1])\n",
    "                # print(f\"Reading basket {i} with {len(data)} bytes at entry offset {basket_entryoffsets[i]}:{basket_entryoffsets[i+1]}\")\n",
    "                compressed_content, border = basket_to_compcont_border(data, dtype, put_stops[i] - put_starts[i])\n",
    "                compressed_contents.append(compressed_content)\n",
    "                borders.append(border)\n",
    "            \n",
    "            branch_metadata = {}\n",
    "            branch_metadata[\"N_baskets\"] = N_baskets\n",
    "            branch_metadata[\"content\"] = content\n",
    "            branch_metadata[\"compressed_contents\"] = compressed_contents\n",
    "            branch_metadata[\"borders\"] = borders\n",
    "            branch_metadata[\"put_starts\"] = put_starts\n",
    "            branch_metadata[\"put_stops\"] = put_stops\n",
    "            branches_metadatas[branch.name] = branch_metadata\n",
    "\n",
    "    for branch_name in branches_metadatas.keys():\n",
    "        metadata = branches_metadatas[branch_name]\n",
    "        compressed_contents = metadata[\"compressed_contents\"]\n",
    "        \n",
    "    all_compressed_content = [compressed_content \n",
    "                                  for branch_name in branches_metadatas.keys() \n",
    "                                  for compressed_content in branches_metadatas[branch_name][\"compressed_contents\"]\n",
    "                             ]\n",
    "\n",
    "    codec = NvCompBatchCodec(\"zstd\")\n",
    "    all_decompressed_content = codec.decode_batch(all_compressed_content)\n",
    "    \n",
    "    for branch in branches:\n",
    "        # Get metadata\n",
    "        branch_metadata = branches_metadatas[branch.name]\n",
    "        N_baskets = branch_metadata[\"N_baskets\"]\n",
    "        content = branch_metadata[\"content\"]\n",
    "        put_starts = branch_metadata[\"put_starts\"]\n",
    "        put_stops = branch_metadata[\"put_stops\"]\n",
    "        borders = branch_metadata[\"borders\"]\n",
    "        \n",
    "        if isinstance(branch.interpretation, uproot.interpretation.jagged.AsJagged):\n",
    "            counts = branch_metadata[\"counts\"]\n",
    "            #Grab chunks of data\n",
    "            branch_decompressed_contents = all_decompressed_content[:N_baskets]\n",
    "            for i in range(N_baskets):\n",
    "                content[put_starts[i]:put_stops[i]] = branch_decompressed_contents[i].view(dtype)[:borders[i] // dtype.itemsize]\n",
    "                \n",
    "            arrays[branch.name] = ak.unflatten(content.astype(branch.interpretation.content.to_dtype), counts)\n",
    "            # Slice out used chunks\n",
    "            all_decompressed_content = all_decompressed_content[N_baskets:]\n",
    "            \n",
    "        elif isinstance(branch.interpretation, uproot.interpretation.numerical.AsDtype):\n",
    "            branch_decompressed_contents = all_decompressed_content[:N_baskets]\n",
    "            for i in range(N_baskets):\n",
    "                content[put_starts[i]:put_stops[i]] = decompressed_contents[i].view(dtype)[:borders[i] // dtype.itemsize]\n",
    "\n",
    "            arrays[branch.name] = content.astype(branch.interpretation.to_dtype)\n",
    "            # Slice out used chunks\n",
    "            all_decompressed_content = all_decompressed_content[N_baskets:]\n",
    "    \n",
    "    return(arrays)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fda7ed74-c3e0-4763-9b11-5ed11e21f092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "636 ms ± 5.45 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "# /store/user/IDAP/RunIISummer20UL18NanoAODv9/TTToSemiLeptonic_TuneCP5_13TeV-powheg-pythia8/NANOAODSIM/20UL18JMENano_106X_upgrade2018_realistic_v16_L1v1-v1/40000/BCB3E2FC-D575-0341-A211-5C9A8D8798B9.root\n",
    "filename = \"/home/fstrug/uscmshome/nobackup/GPU/kvikio_playground/TTToSemiLeptonic_UL18JMENanoAOD-zstd.root\"\n",
    "file = uproot.open(filename)\n",
    "tree = file[\"Events\"]\n",
    "branches = [tree[\"Muon_pt\"]]\n",
    "with open(filename, \"rb\") as rawfile:\n",
    "    myarrays = GPU_all_baskets_to_array(rawfile, branches)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "049cbf1a-f5bf-41a7-b5ee-f4a9ea8bc690",
   "metadata": {},
   "outputs": [],
   "source": [
    "for branch in branches:\n",
    "    assert ak.all(myarrays[branch.name] == branch.array(library=\"ak\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09277c7a-77f4-4553-aafe-c71e86691ee6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-kvikio-env]",
   "language": "python",
   "name": "conda-env-.conda-kvikio-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
